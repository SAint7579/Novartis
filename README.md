# Gene Expression Perturbation Prediction

Deep learning models for predicting gene expression changes after compound perturbations.

## Project Structure

```
Novartis/
â”œâ”€â”€ Dataset/
â”‚   â”œâ”€â”€ csv/
â”‚   â”‚   â””â”€â”€ HEK293T_Counts.csv          # Gene expression counts
â”‚   â”œâ”€â”€ HEK293T_MetaData.xlsx           # Sample metadata
â”‚   â””â”€â”€ SMILES.txt                      # Compound SMILES strings
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ autoencoder/
â”‚   â”‚   â”œâ”€â”€ vae/                        # Standard VAE
â”‚   â”‚   â”‚   â”œâ”€â”€ model.py
â”‚   â”‚   â”‚   â”œâ”€â”€ train.py
â”‚   â”‚   â”‚   â””â”€â”€ utils.py
â”‚   â”‚   â”œâ”€â”€ contrastive_vae/            # VAE with InfoNCE loss
â”‚   â”‚   â”‚   â”œâ”€â”€ model.py                # ContrastiveVAE architecture
â”‚   â”‚   â”‚   â”œâ”€â”€ loss.py                 # Standard InfoNCE
â”‚   â”‚   â”‚   â”œâ”€â”€ loss_logfc.py           # LogFC-weighted InfoNCE
â”‚   â”‚   â”‚   â”œâ”€â”€ dataset.py
â”‚   â”‚   â”‚   â”œâ”€â”€ train.py
â”‚   â”‚   â”‚   â””â”€â”€ utils.py
â”‚   â”‚   â””â”€â”€ triplet_vae/                # VAE with triplet/quadruplet loss
â”‚   â”‚       â”œâ”€â”€ model.py                # TripletVAE architecture
â”‚   â”‚       â”œâ”€â”€ loss.py                 # Quadruplet hinge loss
â”‚   â”‚       â”œâ”€â”€ loss_infonce.py         # InfoNCE variant
â”‚   â”‚       â”œâ”€â”€ loss_fast.py            # Optimized version
â”‚   â”‚       â”œâ”€â”€ dataset.py
â”‚   â”‚       â”œâ”€â”€ dataset_fast.py
â”‚   â”‚       â””â”€â”€ train.py
â”‚   â”‚
â”‚   â””â”€â”€ diffusion/
â”‚       â”œâ”€â”€ smiles_encoder.py           # Pre-trained ChemBERTa
â”‚       â”œâ”€â”€ diffusion_model.py          # Conditional diffusion (DDPM)
â”‚       â””â”€â”€ linear_baseline.py          # Simple MLP baseline
â”‚
â”œâ”€â”€ models/                              # Trained model checkpoints
â”‚   â”œâ”€â”€ vae_hek293t_best.pt
â”‚   â”œâ”€â”€ contrastive_vae_hek293t_best.pt
â”‚   â”œâ”€â”€ contrastive_vae_logfc_hek293t_best.pt
â”‚   â”œâ”€â”€ triplet_vae_hek293t_best.pt
â”‚   â”œâ”€â”€ triplet_vae2_hek293t_best.pt
â”‚   â”œâ”€â”€ diffusion_perturbation_best.pt
â”‚   â””â”€â”€ linear_perturbation_best.pt
â”‚
â”œâ”€â”€ evaluation/                          # Evaluation scripts
â”‚   â”œâ”€â”€ run_all_evals.py                # Master script (runs all)
â”‚   â”œâ”€â”€ visualize_all_models.py         # Latent space plots
â”‚   â”œâ”€â”€ evaluate_topk_fast.py           # Top-k retrieval metrics
â”‚   â”œâ”€â”€ evaluate_perturbation_prediction.py  # Perturbation accuracy
â”‚   â”œâ”€â”€ evaluate_perturbation_models.py # Diffusion/linear comparison
â”‚   â”œâ”€â”€ evaluate_latent_retrieval.py    # Full top-k evaluation
â”‚   â””â”€â”€ plot_volcano.py                 # Volcano plots
â”‚
â”œâ”€â”€ results/                             # Evaluation results
â”‚   â”œâ”€â”€ perturbation_prediction_accuracy.csv
â”‚   â”œâ”€â”€ perturbation_model_comparison.csv
â”‚   â”œâ”€â”€ perturbation_per_treatment.xlsx
â”‚   â”œâ”€â”€ topk_metrics.csv
â”‚   â””â”€â”€ volcano_*.png
â”‚
â”œâ”€â”€ latent_plots/                        # Latent space visualizations
â”‚   â”œâ”€â”€ *_latent.png                    # PCA plots
â”‚   â””â”€â”€ *_latent_tsne.png               # t-SNE plots
â”‚
â””â”€â”€ Training scripts (project root)
    â”œâ”€â”€ train_vae_hek293t.py
    â”œâ”€â”€ train_contrastive_vae_hek293t.py
    â”œâ”€â”€ train_contrastive_vae_logfc_hek293t.py
    â”œâ”€â”€ train_triplet_vae_hek293t.py
    â”œâ”€â”€ train_diffusion_perturbation.py
    â””â”€â”€ train_linear_perturbation.py
```

## Models

### 1. **Standard VAE**
- Basic variational autoencoder
- Loss: Reconstruction + KL divergence
- No replicate information

### 2. **Contrastive VAE** â­
- Adds InfoNCE contrastive learning
- Groups replicates together in latent space
- Projection head prevents task interference
- **Best performer** for replicate retrieval

### 3. **Contrastive VAE + LogFC**
- Same as Contrastive VAE
- InfoNCE weighted by logFC similarity
- Incorporates biological effect size

### 4. **Triplet VAE**
- Quadruplet loss (anchor, positive, DMSO neg, compound neg)
- LogFC-weighted with cosine distance
- Explicit triplet mining

### 5. **Diffusion Model** ğŸš€
- Conditional DDPM in VAE latent space
- Inputs: SMILES (ChemBERTa) + baseline latent + cell line + concentration
- Cross-attention conditioning
- Predicts perturbation from compound structure

### 6. **Linear Baseline**
- Simple MLP: (baseline, SMILES) â†’ perturbation
- Baseline for diffusion comparison

## Quick Start

### Install Dependencies
```bash
pip install pandas numpy torch scikit-learn matplotlib seaborn openpyxl
pip install transformers tokenizers sentencepiece  # For diffusion
```

### Train Models
```bash
# VAE models
python train_contrastive_vae_hek293t.py
python train_contrastive_vae_logfc_hek293t.py

# Diffusion models (requires VAE trained first)
python train_diffusion_perturbation.py
python train_linear_perturbation.py
```

### Evaluate All Models
```bash
python evaluation/run_all_evals.py
```

Or individual evaluations:
```bash
python evaluation/visualize_all_models.py        # Latent space plots
python evaluation/evaluate_topk_fast.py          # Top-k retrieval
python evaluation/evaluate_perturbation_prediction.py  # Perturbation accuracy
python evaluation/evaluate_perturbation_models.py     # Diffusion comparison
```

### Volcano Plots
```bash
# VAE prediction
python evaluation/plot_volcano.py \
    --model models/contrastive_vae_hek293t_best.pt \
    --treatment HY_50946

# Diffusion prediction
python evaluation/plot_volcano.py \
    --model models/contrastive_vae_hek293t_best.pt \
    --diffusion models/diffusion_perturbation_best.pt \
    --smiles Dataset/SMILES.txt \
    --treatment HY_50946
```

## Key Results

### Top-k Replicate Retrieval
| Model | Top-1 Acc | Top-3 Acc | Top-5 Acc |
|-------|-----------|-----------|-----------|
| Ours | 0.25 | 0.30 | 0.32 |
| Contrastive VAE | 0.12 | 0.13 | 0.13 |
| VAE | 0.08 | 0.10 | 0.11 |

### Perturbation Prediction (LogFC Correlation)
| Model | Correlation | Uses SMILES? | Method |
|-------|-------------|--------------|--------|
| Ours | 0.212 | âŒ | Trains MLP per treatment |
| Contrastive VAE | 0.155 | âŒ | Trains MLP per treatment |
| VAE | 0.112 | âŒ | Trains MLP per treatment |
| Diffusion | TBD | âœ… | Zero-shot from SMILES |
| Linear | TBD | âœ… | Zero-shot from SMILES |

## Architecture Highlights

### Contrastive VAE
```
Input (40778 genes) â†’ Encoder â†’ mu (64D) â†’ Decoder â†’ Reconstruction
                                  â†“
                            Projection (128D) â†’ InfoNCE Loss
```

### Diffusion Model
```
Baseline Latent (64D) â”€â”
                       â”œâ”€â†’ [Concat] â†’ Condition (448D)
SMILES (ChemBERTa) â”€â”€â”€â”€â”¤
Cell Line (10D) â”€â”€â”€â”€â”€â”€â”€â”¤
Concentration (1D) â”€â”€â”€â”€â”˜
                       â†“
        [Cross-Attention Diffusion] (512D hidden, 8 heads)
                       â†“
              Post-Perturbation Latent (64D)
```

## Evaluation Metrics

1. **Top-k Accuracy**: Can the model retrieve replicates?
2. **Top-k Precision**: How many retrieved neighbors are true replicates?
3. **LogFC Correlation**: How well does predicted logFC match ground truth?
4. **Latent MSE**: Prediction error in latent space
5. **Expression MSE**: Prediction error in gene expression space

## Files Overview

### Training
- `train_*_hek293t.py` - Train individual models
- All save to `models/*.pt`

### Evaluation
- `evaluation/run_all_evals.py` - **Run this for complete evaluation**
- Results save to `results/*.csv` and `results/*.xlsx`

### Key Dependencies
- PyTorch
- Transformers (HuggingFace)
- pandas, numpy, scipy
- matplotlib, seaborn
- openpyxl (for Excel output)

